{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we will design the full graph using 2 tools: qr_mysql, qr_sqlite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOOL with MySQL**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối tới MySQL qua URI\n",
    "db_uri = \"mysql+pymysql://root:Kiet1006@localhost:3306/Cost_Central_Monitor\"\n",
    "dbm = SQLDatabase.from_uri(db_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.schema import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing import List\n",
    "load_dotenv()\n",
    "azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "gpt_deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "temperature=0,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **new tool mysql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "\n",
    "mysql_llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,  \n",
    "    azure_endpoint=azure_endpoint,  \n",
    "    api_key=api_key,  \n",
    "    azure_deployment=gpt_deployment_name,  \n",
    "    model_name=gpt_deployment_name,  \n",
    "    temperature=0.0  \n",
    ")\n",
    "\n",
    "# Khởi tạo tác nhân một lần để tái sử dụng\n",
    "mysql_agent = create_sql_agent(mysql_llm, db=dbm, agent_type=\"openai-tools\", verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "@tool\n",
    "def qr_mysql(query: str):\n",
    "    \"\"\"\n",
    "    Query the Cost Central Monitor database and access all relevant data from its tables.Input should be a SQL query.\n",
    "    Use ONLY the tables and schema from this database.\n",
    "    Do not infer or query data from other databases.\n",
    "    \"\"\"\n",
    "    # Gọi tác nhân để thực thi truy vấn\n",
    "    result = mysql_agent.invoke({\"input\": query})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_mysql.invoke(\"How many Projects are there?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOOL with SQLite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use raw string for the path\n",
    "db_path = r\"..\\data\\sqldb.db\"\n",
    "\n",
    "# Option 2: Use forward slashes for the path\n",
    "# db_path = \"data/sqldb.db\"\n",
    "\n",
    "dbl = SQLDatabase.from_uri(f\"sqlite:///{db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc tệp SQL\n",
    "with open(\"..\\data\\sql\\langchain.sql\", \"r\") as file:\n",
    "    sql_script = file.read()\n",
    "\n",
    "# Chia tệp SQL thành các câu lệnh riêng biệt\n",
    "sql_commands = sql_script.split(';')  # Tách câu lệnh bằng dấu ;\n",
    "\n",
    "# Thực thi từng câu lệnh SQL\n",
    "for command in sql_commands:\n",
    "    command = command.strip()\n",
    "    if command:  # Nếu câu lệnh không rỗng\n",
    "        dbl.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **new tool sqlite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "\n",
    "sqlite_llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,  \n",
    "    azure_endpoint=azure_endpoint,  \n",
    "    api_key=api_key,  \n",
    "    azure_deployment=gpt_deployment_name,  \n",
    "    model_name=gpt_deployment_name,  \n",
    "    temperature=0.0  \n",
    ")\n",
    "# Khởi tạo tác nhân một lần để tái sử dụng\n",
    "sqlite_agent = create_sql_agent(sqlite_llm, db=dbl, agent_type=\"openai-tools\", verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "@tool\n",
    "def qr_sqlite(query: str):\n",
    "    \"\"\"\n",
    "    Query the Langchain database and access all relevant data from its tables. Input should be in SQL query.\n",
    "    Use ONLY the tables and schema from this database.\n",
    "    Do not infer or query data from other databases.\n",
    "    \"\"\"\n",
    "    # Gọi tác nhân để thực thi truy vấn\n",
    "    result = sqlite_agent.invoke({\"input\": query})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **check tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gọi hàm\n",
    "qr_sqlite.invoke(\"How many Providers are there?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_sqlite.invoke(\"displaying cost alerts for projects where the cost exceeds the alert threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_sqlite.invoke(\"List the total Cost per Project. Which Project has the most cost?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Wrap up the tools into a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [qr_mysql, qr_sqlite]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load the LLM for the primary agent and bind it with the tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_primary = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,  \n",
    "    azure_endpoint=azure_endpoint,  \n",
    "    api_key=api_key,  \n",
    "    azure_deployment=gpt_deployment_name,  \n",
    "    model_name=gpt_deployment_name,  \n",
    "    temperature=0.0  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell the LLM which tools it can call\n",
    "llm_with_tools = llm_primary.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Initialize the Graph State**\n",
    "\n",
    "Define our StateGraph's state as a typed dictionary containing an append-only list of messages. These messages form the chat history, which is all the state our chatbot needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Define the Graph Nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 First node: chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a function that will run the tools when they are needed. To do this, we'll add the tools to a new node.\n",
    "\n",
    "In the example below, we'll build a BasicToolNode. This node will check the latest message and, if it contains a request to use a tool, it will run the appropriate tool. This works because many language models (like Anthropic, OpenAI, and Google Gemini) support tool usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Second node: BasicToolNode that runs the appropriate tool based on the primary agent's output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "# tool_node = BasicToolNode(tools=[query_mysql, query_sqlite])\n",
    "tool_node = BasicToolNode(tools=[qr_mysql, qr_sqlite])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Define the entry point and graph edges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approach 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessagesState\n",
    "from typing import Literal\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    should_continue,\n",
    "    [\"tools\", END],\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Compile the graph**\n",
    "\n",
    "- In this step, we can add a memory to our graph as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Plot the compiled graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Execute the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Which project exceeded the cost threshold in June 2024 in both database?\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
